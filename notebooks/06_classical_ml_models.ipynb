{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Classical ML Models (SVM, Random Forest, GMM)\n",
    "\n",
    "Train and evaluate classical machine learning models for audio classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Flattened Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten 2D features to 1D for classical ML\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(f\"Flattened feature shapes:\")\n",
    "print(f\"  X_train_flat: {X_train_flat.shape}\")\n",
    "print(f\"  X_val_flat: {X_val_flat.shape}\")\n",
    "print(f\"  X_test_flat: {X_test_flat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUPPORT VECTOR MACHINE (SVM)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train SVM\n",
    "print(\"Training SVM...\")\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)\n",
    "svm_model.fit(X_train_flat, y_train)\n",
    "print(\"✓ SVM training complete\\n\")\n",
    "\n",
    "# Validation accuracy\n",
    "predictions_svm_val = svm_model.predict(X_val_flat)\n",
    "accuracy_svm_val = accuracy_score(y_val, predictions_svm_val)\n",
    "\n",
    "# Test accuracy\n",
    "predictions_svm = svm_model.predict(X_test_flat)\n",
    "accuracy_svm = accuracy_score(y_test, predictions_svm)\n",
    "precision_svm = precision_score(y_test, predictions_svm, average='weighted')\n",
    "recall_svm = recall_score(y_test, predictions_svm, average='weighted')\n",
    "f1_svm = f1_score(y_test, predictions_svm, average='weighted')\n",
    "\n",
    "cm_svm = confusion_matrix(y_test, predictions_svm)\n",
    "\n",
    "print(\"SVM Results:\")\n",
    "print(f\"  Validation Accuracy: {accuracy_svm_val:.4f}\")\n",
    "print(f\"  Test Accuracy:  {accuracy_svm:.4f}\")\n",
    "print(f\"  Precision: {precision_svm:.4f}\")\n",
    "print(f\"  Recall:    {recall_svm:.4f}\")\n",
    "print(f\"  F1-score:  {f1_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_flat, y_train)\n",
    "print(\"✓ Random Forest training complete\\n\")\n",
    "\n",
    "# Validation accuracy\n",
    "predictions_rf_val = rf_model.predict(X_val_flat)\n",
    "accuracy_rf_val = accuracy_score(y_val, predictions_rf_val)\n",
    "\n",
    "# Test accuracy\n",
    "predictions_rf = rf_model.predict(X_test_flat)\n",
    "accuracy_rf = accuracy_score(y_test, predictions_rf)\n",
    "precision_rf = precision_score(y_test, predictions_rf, average='weighted')\n",
    "recall_rf = recall_score(y_test, predictions_rf, average='weighted')\n",
    "f1_rf = f1_score(y_test, predictions_rf, average='weighted')\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, predictions_rf)\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"  Validation Accuracy: {accuracy_rf_val:.4f}\")\n",
    "print(f\"  Test Accuracy:  {accuracy_rf:.4f}\")\n",
    "print(f\"  Precision: {precision_rf:.4f}\")\n",
    "print(f\"  Recall:    {recall_rf:.4f}\")\n",
    "print(f\"  F1-score:  {f1_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GAUSSIAN MIXTURE MODEL (GMM)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Apply PCA\n",
    "print(\"Applying PCA (n_components=128)...\")\n",
    "n_components_pca = 128\n",
    "pca = PCA(n_components=n_components_pca, random_state=42)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_flat)\n",
    "X_val_pca = pca.transform(X_val_flat)\n",
    "X_test_pca = pca.transform(X_test_flat)\n",
    "print(f\"✓ PCA complete. Shape: {X_train_pca.shape}\\n\")\n",
    "\n",
    "# Train per-class GMM\n",
    "print(\"Training GMM models for each class...\")\n",
    "gmm_models = {}\n",
    "\n",
    "for class_name in y_train.unique():\n",
    "    X_train_class = X_train_pca[y_train == class_name]\n",
    "    gmm = GaussianMixture(n_components=1, covariance_type='diag', random_state=42)\n",
    "    gmm.fit(X_train_class)\n",
    "    gmm_models[class_name] = gmm\n",
    "\n",
    "print(f\"✓ Trained GMM for {len(gmm_models)} classes\\n\")\n",
    "\n",
    "# Validation predictions\n",
    "def predict_gmm(X, gmm_models):\n",
    "    predictions = []\n",
    "    for sample in X:\n",
    "        sample = sample.reshape(1, -1)\n",
    "        likelihoods = {cls: gmm.score(sample) for cls, gmm in gmm_models.items()}\n",
    "        predicted_class = max(likelihoods, key=likelihoods.get)\n",
    "        predictions.append(predicted_class)\n",
    "    return np.array(predictions)\n",
    "\n",
    "predictions_gmm_val = predict_gmm(X_val_pca, gmm_models)\n",
    "accuracy_gmm_val = accuracy_score(y_val, predictions_gmm_val)\n",
    "\n",
    "# Test predictions\n",
    "predictions_gmm = predict_gmm(X_test_pca, gmm_models)\n",
    "accuracy_gmm = accuracy_score(y_test, predictions_gmm)\n",
    "precision_gmm = precision_score(y_test, predictions_gmm, average='weighted')\n",
    "recall_gmm = recall_score(y_test, predictions_gmm, average='weighted')\n",
    "f1_gmm = f1_score(y_test, predictions_gmm, average='weighted')\n",
    "\n",
    "cm_gmm = confusion_matrix(y_test, predictions_gmm)\n",
    "\n",
    "print(\"GMM Results:\")\n",
    "print(f\"  Validation Accuracy: {accuracy_gmm_val:.4f}\")\n",
    "print(f\"  Test Accuracy:  {accuracy_gmm:.4f}\")\n",
    "print(f\"  Precision: {precision_gmm:.4f}\")\n",
    "print(f\"  Recall:    {recall_gmm:.4f}\")\n",
    "print(f\"  F1-score:  {f1_gmm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['SVM', 'Random Forest', 'GMM'],\n",
    "    'Val Accuracy': [accuracy_svm_val, accuracy_rf_val, accuracy_gmm_val],\n",
    "    'Test Accuracy': [accuracy_svm, accuracy_rf, accuracy_gmm],\n",
    "    'Precision': [precision_svm, precision_rf, precision_gmm],\n",
    "    'Recall': [recall_svm, recall_rf, recall_gmm],\n",
    "    'F1-Score': [f1_svm, f1_rf, f1_gmm]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSICAL ML MODELS - COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\nprint(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# SVM CM\n",
    "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', ax=axes[0], cbar=False,\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "axes[0].set_title('SVM Confusion Matrix')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# RF CM\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', ax=axes[1], cbar=False,\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "axes[1].set_title('Random Forest Confusion Matrix')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "# GMM CM\n",
    "sns.heatmap(cm_gmm, annot=True, fmt='d', cmap='Oranges', ax=axes[2], cbar=False,\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "axes[2].set_title('GMM Confusion Matrix')\n",
    "axes[2].set_ylabel('True Label')\n",
    "axes[2].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Accuracy comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(3)\n",
    "width = 0.35\n",
    "\n",
    "val_accs = [accuracy_svm_val, accuracy_rf_val, accuracy_gmm_val]\n",
    "test_accs = [accuracy_svm, accuracy_rf, accuracy_gmm]\n",
    "\n",
    "ax.bar(x - width/2, val_accs, width, label='Validation', color='skyblue', edgecolor='navy')\n",
    "ax.bar(x + width/2, test_accs, width, label='Test', color='coral', edgecolor='darkred')\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Classical ML Models - Validation vs Test Accuracy')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['SVM', 'Random Forest', 'GMM'])\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1.1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
