{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Feature Extraction (MFCC)\n",
    "\n",
    "Extract MFCC features from audio files and prepare data for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"✓ All imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MFCC Feature Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_features(waveform, sample_rate, n_mfcc=40, n_fft=400, hop_length=160, max_padding=176400):\n",
    "    \"\"\"\n",
    "    Extracts MFCC features from an audio waveform.\n",
    "\n",
    "    Args:\n",
    "        waveform (numpy.ndarray or torch.Tensor): The audio waveform.\n",
    "        sample_rate (int): The sample rate of the waveform.\n",
    "        n_mfcc (int): Number of MFCC coefficients to retain.\n",
    "        n_fft (int): Size of the FFT window.\n",
    "        hop_length (int): Number of samples between successive frames.\n",
    "        max_padding (int): Maximum length to pad or truncate the waveform to.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The MFCC features.\n",
    "    \"\"\"\n",
    "    # Convert numpy array to torch tensor if needed\n",
    "    if isinstance(waveform, np.ndarray):\n",
    "        waveform = torch.from_numpy(waveform).float()\n",
    "    \n",
    "    # Ensure waveform is 2D: (channels, samples)\n",
    "    if waveform.ndim == 1:\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "    \n",
    "    # Ensure waveform is mono if it has multiple channels\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Resample if sample rate is not 16000\n",
    "    if sample_rate != 16000:\n",
    "        resampler = T.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "        sample_rate = 16000\n",
    "\n",
    "    # Pad or truncate the waveform to a fixed length\n",
    "    if waveform.shape[-1] < max_padding:\n",
    "        padding = max_padding - waveform.shape[-1]\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "    elif waveform.shape[-1] > max_padding:\n",
    "        waveform = waveform[..., :max_padding]\n",
    "\n",
    "    # Extract MFCC\n",
    "    mfcc_transform = T.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=n_mfcc,\n",
    "        melkwargs={\n",
    "            \"n_fft\": n_fft,\n",
    "            \"hop_length\": hop_length,\n",
    "            \"n_mels\": 128,\n",
    "            \"f_min\": 0,\n",
    "            \"f_max\": sample_rate / 2,\n",
    "        },\n",
    "    )\n",
    "    mfcc_features = mfcc_transform(waveform)\n",
    "    return mfcc_features.squeeze(0)\n",
    "\n",
    "print(\"✓ MFCC extraction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features from All Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store extracted features and labels\n",
    "extracted_features = []\n",
    "\n",
    "# Iterate through the myVoices list and extract features\n",
    "print(\"Extracting MFCC features...\")\n",
    "for i, audio_item in enumerate(myVoices):\n",
    "    try:\n",
    "        waveform = audio_item['waveform']\n",
    "        sample_rate = audio_item['sample_rate']\n",
    "        category = audio_item['category']\n",
    "\n",
    "        # Extract MFCC features\n",
    "        features = extract_mfcc_features(waveform, sample_rate)\n",
    "\n",
    "        # Append features and category to the list\n",
    "        extracted_features.append({\n",
    "            'features': features,\n",
    "            'category': category\n",
    "        })\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(myVoices)} files\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not extract features for {audio_item.get('path', 'an audio file')}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"✓ Feature extraction complete! Extracted {len(extracted_features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the extracted features and labels\n",
    "extracted_features_df = pd.DataFrame(extracted_features)\n",
    "\n",
    "# Display the first few rows of the DataFrame and its shape\n",
    "print(\"Extracted Features DataFrame:\")\n",
    "display(extracted_features_df.head())\n",
    "print(f\"\\nShape of the extracted features DataFrame: {extracted_features_df.shape}\")\n",
    "print(f\"Feature shape per sample: {extracted_features_df['features'].iloc[0].shape}\")\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(extracted_features_df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "extracted_features_df_shuffled = extracted_features_df.sample(\n",
    "    frac=1.0, \n",
    "    random_state=42\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"✓ Data shuffled\")\nprint(f\"Original class distribution:\")\nprint(extracted_features_df_shuffled['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X = extracted_features_df_shuffled['features']\n",
    "y = extracted_features_df_shuffled['category']\n",
    "\n",
    "# Convert list of tensors to NumPy array\n",
    "if all(item.shape == X.iloc[0].shape for item in X):\n",
    "    X = torch.stack(list(X)).numpy()\n",
    "else:\n",
    "    raise ValueError(\"Feature tensor shapes are inconsistent, cannot stack.\")\n",
    "\n",
    "print(f\"Stacked feature shape: {X.shape}\")\n",
    "\n",
    "# First split - Train vs Rest\n",
    "X_train_raw, X_temp, y_train_raw, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ First split done\")\n",
    "print(f\"Train (raw): {len(X_train_raw)} samples\")\n",
    "print(f\"Temp: {len(X_temp)} samples\")\n",
    "\n",
    "# Second split - Val vs Test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    random_state=42, \n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Second split done\")\n",
    "print(f\"Validation: {len(X_val)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set BEFORE balancing:\")\nprint(pd.Series(y_train_raw).value_counts())\n",
    "\n",
    "# Find minimum class count\n",
    "class_counts = Counter(y_train_raw)\n",
    "min_samples = min(class_counts.values())\n",
    "\n",
    "# Undersample each class to min_samples\n",
    "balanced_indices = []\n",
    "for class_label in class_counts.keys():\n",
    "    class_indices = np.where(y_train_raw == class_label)[0]\n",
    "    sampled_indices = np.random.choice(\n",
    "        class_indices, \n",
    "        size=min_samples, \n",
    "        replace=False\n",
    "    )\n",
    "    balanced_indices.extend(sampled_indices)\n",
    "\n",
    "# Shuffle balanced indices\n",
    "np.random.shuffle(balanced_indices)\n",
    "\n",
    "# Create balanced training set\n",
    "X_train = X_train_raw[balanced_indices]\n",
    "y_train = y_train_raw.iloc[balanced_indices].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTraining set AFTER undersampling:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL DATASET SPLITS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training set:   {X_train.shape[0]} samples (BALANCED)\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples (ORIGINAL)\")\n",
    "print(f\"Test set:       {X_test.shape[0]} samples (ORIGINAL)\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
